#!/usr/bin/env python3
import re
import time
import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urlparse, unquote, urljoin
import sys
import subprocess
import math
import numpy as np

def check_dependencies():
    """Ensure required packages are installed"""
    try:
        import openpyxl
        return True
    except ImportError:
        print("\n❌ Missing required package: openpyxl")
        print("This package is needed for Excel export functionality")
        choice = input("Install it now? (y/n): ").lower()
        if choice == 'y':
            try:
                subprocess.check_call([sys.executable, "-m", "pip", "install", "openpyxl"])
                print("✅ openpyxl installed successfully!")
                return True
            except:
                print("🚨 Installation failed. CSV export will still work")
                return False
        return False

def clean_sheet_name(name):
    """Clean invalid characters from Excel sheet names"""
    name = re.sub(r'[\\/*?:\[\]]', '', name)
    return name[:31]  # Excel limit is 31 characters

def generate_unique_filename(base_path, format):
    """Generate unique filename to prevent overwriting"""
    counter = 0
    path_no_ext, ext = os.path.splitext(base_path)
    new_path = base_path
    
    while os.path.exists(new_path):
        counter += 1
        new_path = f"{path_no_ext}_{counter}.{format}"
    
    return new_path

def extract_tables_from_container(container, table_classes=None):
    """Extract tables from a specific container element"""
    if table_classes:
        tables = []
        for cls in table_classes:
            tables.extend(container.find_all('table', class_=cls))
        return tables
    return container.find_all('table')

def extract_links_from_container(container, link_classes=None):
    """Extract links from a specific container element"""
    links = []
    for a in container.find_all('a', href=True):
        # Skip empty links
        if not a['href'] or a['href'].startswith(('javascript:', 'mailto:', 'tel:')):
            continue
            
        if link_classes:
            for cls in link_classes:
                if cls in a.get('class', []):
                    links.append({
                        'url': a['href'],
                        'text': a.get_text(strip=True),
                        'class': cls
                    })
                    break
        else:
            links.append({
                'url': a['href'],
                'text': a.get_text(strip=True)
            })
    return links

def manual_table_extraction(table):
    """Robust table extraction that handles various table structures"""
    try:
        # Extract headers from thead or first row
        headers = []
        header_row = None
        
        # 1. Look for thead section
        thead = table.find('thead')
        if thead:
            for row in thead.find_all('tr'):
                for th in row.find_all(['th', 'td']):
                    headers.append(th.get_text(strip=True))
        
        # 2. If no thead, look for th elements in the table
        if not headers:
            for th in table.find_all('th'):
                headers.append(th.get_text(strip=True))
        
        # 3. If still no headers, use first row as headers
        if not headers:
            first_row = table.find('tr')
            if first_row:
                header_row = first_row
                for cell in first_row.find_all(['th', 'td']):
                    headers.append(cell.get_text(strip=True))
        
        # Extract data rows
        rows = []
        tbody = table.find('tbody') or table  # Fallback to entire table if no tbody
        
        for row in tbody.find_all('tr'):
            # Skip header row if we identified it
            if row == header_row:
                continue
                
            cols = row.find_all(['td', 'th'])
            if cols:
                row_data = [col.get_text(strip=True) for col in cols]
                rows.append(row_data)
        
        # Find max columns to ensure consistent shape
        max_columns = max(len(row) for row in rows) if rows else 0
        if headers:
            if len(headers) > max_columns:
                headers = headers[:max_columns]
            elif len(headers) < max_columns:
                headers += [f"Column_{i+1}" for i in range(len(headers), max_columns)]
        
        # Pad rows to match column count
        padded_rows = []
        for row in rows:
            if len(row) < max_columns:
                row += [''] * (max_columns - len(row))
            padded_rows.append(row)
        
        # Create DataFrame
        if headers and padded_rows:
            return pd.DataFrame(padded_rows, columns=headers)
        elif padded_rows:
            return pd.DataFrame(padded_rows)
        else:
            return None
    except Exception as e:
        print(f"    ⚠️ Table extraction error: {str(e)}")
        return None

def extract_content_robust(url, content_type, classes=None, ids=None, link_classes=None):
    """Reliable content extraction that works for complex sites"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Handle container IDs if specified
        containers = []
        if ids:
            for container_id in ids:
                container = soup.find(id=container_id)
                if container:
                    containers.append(container)
        else:
            containers = [soup]
        
        results = []
        for container in containers:
            if content_type == 'tables':
                tables = extract_tables_from_container(container, classes)
                for table in tables:
                    df = manual_table_extraction(table)
                    if df is not None and not df.empty:
                        results.append(df)
            elif content_type == 'links':
                links = extract_links_from_container(container, link_classes)
                results.extend(links)
        
        return results
    except Exception as e:
        print(f"⚠️ Error fetching {url}: {str(e)}")
        return []

def extract_url(url, content_type, classes=None, ids=None, link_classes=None, num_pages=1):
    """Extract content from a single URL with pagination support"""
    all_content = []
    page_range = range(1, num_pages + 1) if "{}" in url else [1]
    
    print(f"⏳ Extracting content from {min(page_range)} to {max(page_range)} pages...")
    
    for page in page_range:
        try:
            target_url = url.format(page) if "{}" in url else url
            print(f"  🌐 Fetching page {page}: {target_url}")
            
            content = extract_content_robust(
                target_url, 
                content_type, 
                classes=classes, 
                ids=ids,
                link_classes=link_classes
            )
            
            if not content:
                print(f"    ⚠️ No content found on page {page}")
                continue
            
            # Process tables
            if content_type == 'tables':
                for i, df in enumerate(content):
                    all_content.append(df)
                    print(f"    ✅ Extracted table {i+1} ({len(df)} rows)")
            
            # Process links
            elif content_type == 'links':
                for link in content:
                    # Make absolute URL
                    absolute_url = urljoin(target_url, link['url'])
                    link['absolute_url'] = absolute_url
                all_content.extend(content)
                print(f"    ✅ Extracted {len(content)} links")
                
            time.sleep(1.5)  # Be polite to servers
            
        except Exception as e:
            print(f"🚨 Page {page} failed: {str(e)}")
    
    return all_content

def group_tables(tables):
    """Group consecutive tables with identical headers"""
    if not tables:
        return []
    
    groups = []
    current_group = []
    
    for table in tables:
        if not current_group:
            current_group.append(table)
            continue
            
        # Check if current table has same headers as last table in current group
        prev_headers = list(current_group[-1].columns)
        curr_headers = list(table.columns)
        
        if curr_headers == prev_headers:
            current_group.append(table)
        else:
            groups.append(current_group)
            current_group = [table]
    
    if current_group:
        groups.append(current_group)
        
    return groups

def save_tables_with_gap(tables, output_path, source_url=None):
    """Save tables with 5-row gaps between groups of tables with different headers"""
    try:
        import openpyxl
        from openpyxl.utils.dataframe import dataframe_to_rows
        
        # Create workbook and remove default sheet
        workbook = openpyxl.Workbook()
        if workbook.sheetnames:
            workbook.remove(workbook.active)
        
        # Create sheet with source-based name
        if source_url:
            domain = urlparse(source_url).netloc.replace('www.', '')[:15]
            sheet_name = f"{domain}_Data"
        else:
            sheet_name = "ExtractedData"
        sheet_name = clean_sheet_name(sheet_name)
        worksheet = workbook.create_sheet(sheet_name)
        
        # Group tables by header similarity
        table_groups = group_tables(tables)
        
        # Start writing from row 1
        current_row = 1
        
        for group_idx, group in enumerate(table_groups):
            # Write group label
            worksheet.cell(row=current_row, column=1, value=f"Table Group {group_idx+1}")
            current_row += 1
            
            # Write header (only once per group)
            headers = group[0].columns.tolist()
            for col_idx, header in enumerate(headers, 1):
                worksheet.cell(row=current_row, column=col_idx, value=header)
            current_row += 1
            
            # Write all tables in the group without gaps
            for table in group:
                for row_idx, row in table.iterrows():
                    for col_idx, value in enumerate(row, 1):
                        worksheet.cell(row=current_row, column=col_idx, value=value)
                    current_row += 1
            
            # Add 5-row gap after group
            current_row += 5
        
        # Save workbook
        workbook.save(output_path)
        print(f"💾 Saved Excel with {len(table_groups)} table groups: {os.path.abspath(output_path)}")
        return True
    except Exception as e:
        print(f"🚨 Excel save failed: {str(e)}")
        return False

def save_content(content, content_type, output_path, source_url=None):
    """Save extracted content to appropriate format"""
    if not content:
        print("❌ No content to save")
        return False
    
    # Create directory if needed
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # Handle tables
    if content_type == 'tables':
        if output_path.endswith('.csv'):
            try:
                combined = pd.concat(content, ignore_index=True)
                combined.to_csv(output_path, index=False)
                print(f"💾 Saved CSV: {os.path.abspath(output_path)}")
                return True
            except Exception as e:
                print(f"🚨 CSV save failed: {str(e)}")
                return False
        
        elif output_path.endswith('.xlsx'):
            try:
                import openpyxl
            except ImportError:
                print("❌ Excel support requires openpyxl package")
                print("Run: pip install openpyxl")
                print("Saving as CSV instead")
                csv_path = os.path.splitext(output_path)[0] + '.csv'
                return save_content(content, content_type, csv_path, source_url)
                
            return save_tables_with_gap(content, output_path, source_url)
        
        else:
            print("❌ Unsupported format. Using default: output.xlsx")
            return save_content(content, content_type, "output.xlsx", source_url)
    
    # Handle links
    elif content_type == 'links':
        # Create DataFrame from links
        try:
            df = pd.DataFrame(content)
        except Exception as e:
            print(f"🚨 Link processing failed: {str(e)}")
            return False
        
        if output_path.endswith('.txt'):
            try:
                with open(output_path, 'w') as f:
                    for link in content:
                        f.write(f"{link['absolute_url']}\n")
                print(f"💾 Saved TXT: {os.path.abspath(output_path)}")
                return True
            except Exception as e:
                print(f"🚨 TXT save failed: {str(e)}")
                return False
        
        elif output_path.endswith('.csv'):
            try:
                df.to_csv(output_path, index=False)
                print(f"💾 Saved CSV: {os.path.abspath(output_path)}")
                return True
            except Exception as e:
                print(f"🚨 CSV save failed: {str(e)}")
                return False
            
        elif output_path.endswith('.xlsx'):
            try:
                import openpyxl
            except ImportError:
                print("❌ Excel support requires openpyxl package")
                print("Saving as CSV instead")
                csv_path = os.path.splitext(output_path)[0] + '.csv'
                return save_content(content, content_type, csv_path, source_url)
            
            try:
                with pd.ExcelWriter(output_path) as writer:
                    # Create meaningful sheet name
                    if source_url:
                        domain = urlparse(source_url).netloc.replace('www.', '')[:15]
                        sheet_name = f"{domain}_Links"
                    else:
                        sheet_name = "Links"
                    
                    sheet_name = clean_sheet_name(sheet_name)
                    df.to_excel(writer, sheet_name=sheet_name, index=False)
                    print(f"💾 Saved Excel: {os.path.abspath(output_path)}")
                return True
            except Exception as e:
                print(f"🚨 Excel save failed: {str(e)}")
                return False
        
        else:
            print("❌ Unsupported format. Using default: links.txt")
            return save_content(content, content_type, "links.txt", source_url)
    
    return False

def parse_comma_separated(input_str):
    """Parse comma-separated input into list, handling empty values"""
    if not input_str:
        return None
    items = [item.strip() for item in input_str.split(',') if item.strip()]
    return items if items else None

def main():
    print("\n" + "="*50)
    print("🌐 XTRACT - WEB CONTENT EXTRACTOR".center(50))
    print("="*50)
    
    # Check for required dependencies
    excel_support = check_dependencies()
    
    # Content type selection
    content_type = input("\nExtract tables or links? (t/l): ").strip().lower()
    if content_type not in ['t', 'l']:
        print("\n❌ Invalid selection. Please choose 't' for tables or 'l' for links")
        return
    
    content_type = 'tables' if content_type == 't' else 'links'
    
    # Single vs Multiple URLs
    mode = input("\nSingle URL or Multiple URLs? (1/2): ").strip()
    
    # Process URLs based on mode
    urls = []
    classes = None
    ids = None
    link_classes = None
    
    # Get parameters only once for all URLs
    if content_type == 'tables':
        classes = parse_comma_separated(
            input("\nEnter table class(es) (optional, comma separated):\n> ").strip()
        )
        ids = parse_comma_separated(
            input("\nEnter container ID(s) (optional, comma separated):\n> ").strip()
        )
    else:
        link_classes = parse_comma_separated(
            input("\nEnter link class(es) (optional, comma separated):\n> ").strip()
        )
        ids = parse_comma_separated(
            input("\nEnter container ID(s) (optional, comma separated):\n> ").strip()
        )
    
    if mode == '1':
        # Single URL processing
        url_input = input("\nEnter website URL:\n> ").strip()
        
        # URL logic handling
        if re.search(r'/\d+$', url_input):  # Specific page
            url = url_input
            num_pages = 1
        elif re.search(r'\{(\d+)\}', url_input):  # Page range
            num_pages = int(re.search(r'\{(\d+)\}', url_input).group(1))
            url = re.sub(r'\{\d+\}', '{}', url_input)
        else:  # Single page
            url = url_input
            num_pages = 1
            
        urls.append((url, num_pages))
        
        # Output filename
        default_name = generate_unique_filename(
            urlparse(url).netloc.replace('www.', '')[:20] + f".{'xlsx' if content_type == 'tables' else 'txt'}",
            'xlsx' if content_type == 'tables' else 'txt'
        )
        filename = input(f"\nEnter output filename (default: {default_name}):\n> ").strip() or default_name
        
        # Extract and save
        content = extract_url(
            url, 
            content_type, 
            classes=classes, 
            ids=ids, 
            link_classes=link_classes, 
            num_pages=num_pages
        )
        if content:
            save_content(content, content_type, filename, url)
        else:
            print("\n❌ No content extracted. Check URL and parameters")
        
    elif mode == '2':
        # Multiple URL processing
        print("\nEnter URLs (space/comma separated, multiple lines allowed):")
        print("Press Enter twice when finished")
        lines = []
        while True:
            line = input("> ").strip()
            if line:
                lines.append(line)
            else:
                break
                
        # Combine all lines
        combined_input = " ".join(lines)
        raw_urls = re.split(r'[,\s]+', combined_input)
        urls = []
        
        # Process each URL
        for url in raw_urls:
            if not url:
                continue
                
            if re.search(r'/\d+$', url):  # Specific page
                urls.append((url, 1))
            elif re.search(r'\{(\d+)\}', url):  # Page range
                num_pages = int(re.search(r'\{(\d+)\}', url).group(1))
                base_url = re.sub(r'\{\d+\}', '{}', url)
                urls.append((base_url, num_pages))
            else:  # Single page
                urls.append((url, 1))
        
        print(f"\nFound {len(urls)} URLs to process")
        
        # Output handling
        output_mode = input("\nSave in single file or multiple files? (1/2): ").strip()
        
        if output_mode == '1':  # Single file
            if content_type == 'tables' and not excel_support:
                print("\n❌ Excel support not available. Saving as multiple CSV files instead")
                output_mode = '2'
            else:
                default_ext = 'xlsx' if content_type == 'tables' else 'csv'
                filename = input(f"\nEnter output filename (default: combined.{default_ext}):\n> ").strip()
                if not filename:
                    filename = f"combined.{default_ext}"
                elif '.' not in filename:
                    filename += f".{default_ext}"
                    
                # For tables, save each URL to separate sheet
                if content_type == 'tables':
                    try:
                        import openpyxl
                        workbook = openpyxl.Workbook()
                        workbook.remove(workbook.active)  # Remove default sheet
                        
                        for i, (url, num_pages) in enumerate(urls):
                            print(f"\nProcessing URL {i+1}/{len(urls)}: {url}")
                            content = extract_url(
                                url, content_type, 
                                classes=classes, 
                                ids=ids, 
                                link_classes=link_classes, 
                                num_pages=num_pages
                            )
                            
                            if content:
                                # Create sheet for this URL
                                domain = urlparse(url).netloc.replace('www.', '')[:15]
                                sheet_name = clean_sheet_name(f"{domain}_{i+1}")
                                worksheet = workbook.create_sheet(sheet_name)
                                
                                # Group tables by header similarity
                                table_groups = group_tables(content)
                                
                                # Start writing from row 1
                                current_row = 1
                                
                                for group_idx, group in enumerate(table_groups):
                                    # Write group label
                                    worksheet.cell(row=current_row, column=1, value=f"Table Group {group_idx+1}")
                                    current_row += 1
                                    
                                    # Write header (only once per group)
                                    headers = group[0].columns.tolist()
                                    for col_idx, header in enumerate(headers, 1):
                                        worksheet.cell(row=current_row, column=col_idx, value=header)
                                    current_row += 1
                                    
                                    # Write all tables in the group without gaps
                                    for table in group:
                                        for row_idx, row in table.iterrows():
                                            for col_idx, value in enumerate(row, 1):
                                                worksheet.cell(row=current_row, column=col_idx, value=value)
                                            current_row += 1
                                    
                                    # Add 5-row gap after group
                                    current_row += 5
                                
                                print(f"  💾 Added sheet: {sheet_name}")
                            else:
                                print(f"  ⚠️ No content for {url}")
                        
                        workbook.save(filename)
                        print(f"\n💾 Saved Excel with {len(workbook.sheetnames)} sheets: {os.path.abspath(filename)}")
                    except Exception as e:
                        print(f"🚨 Excel save failed: {str(e)}")
                else:  # Links
                    all_content = []
                    for i, (url, num_pages) in enumerate(urls):
                        print(f"\nProcessing URL {i+1}/{len(urls)}: {url}")
                        content = extract_url(
                            url, content_type, 
                            classes=classes, 
                            ids=ids, 
                            link_classes=link_classes, 
                            num_pages=num_pages
                        )
                        if content:
                            all_content.extend(content)
                    
                    if all_content:
                        save_content(all_content, content_type, filename)
                    else:
                        print("\n❌ No content extracted from any URLs")
                
        if output_mode == '2':  # Multiple files
            output_dir = input("\nEnter output directory (default: current directory):\n> ").strip()
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir)
                
            for i, (url, num_pages) in enumerate(urls):
                print(f"\nProcessing URL {i+1}/{len(urls)}: {url}")
                content = extract_url(
                    url, content_type, 
                    classes=classes, 
                    ids=ids, 
                    link_classes=link_classes, 
                    num_pages=num_pages
                )
                
                if content:
                    # Generate unique filename to prevent overwriting
                    base_name = urlparse(url).netloc.replace('www.', '')[:20] + \
                        f".{'xlsx' if content_type == 'tables' else 'txt'}"
                    if output_dir:
                        base_path = os.path.join(output_dir, base_name)
                    else:
                        base_path = base_name
                    
                    filename = generate_unique_filename(base_path, 'xlsx' if content_type == 'tables' else 'txt')
                    save_content(content, content_type, filename, url)
    
    print("\n✅ Extraction complete! Happy analyzing!")
    print("="*50 + "\n")

if __name__ == "__main__":
    main()
