#!/usr/bin/env python3
import argparse
import requests
import pandas as pd
from bs4 import BeautifulSoup
import re
import time
import os
from tqdm import tqdm

def clean_sheet_name(name):
    return re.sub(r'[\\/*?:\[\]]', '', name)[:30]

def manual_table_extraction(table):
    headers = []
    thead = table.find('thead')
    if thead:
        for th in thead.find_all('th'):
            headers.append(th.get_text(strip=True))
    
    rows = []
    tbody = table.find('tbody')
    if tbody:
        for row in tbody.find_all('tr'):
            cols = row.find_all('td')
            if cols:
                row_data = [col.get_text(strip=True) for col in cols]
                rows.append(row_data)
    
    if headers and rows:
        return pd.DataFrame(rows, columns=headers)
    elif rows:
        return pd.DataFrame(rows)
    else:
        return None

def extract_tables_robust(url, table_class=None):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        tables = soup.find_all('table', class_=table_class) if table_class else soup.find_all('table')
        return tables
    except Exception as e:
        print(f"⚠️ Error fetching {url}: {str(e)}")
        return []

def main():
    parser = argparse.ArgumentParser(description='Extract tables from websites')
    parser.add_argument('url', nargs='?', help='Website URL pattern (use {} for page numbers)')
    args = parser.parse_args()

    print("🔍 Robust Web Table Extractor")
    print("=" * 45)
    
    base_url = args.url if args.url else input("\nEnter website URL pattern (use {} for page number):\n> ")
    num_pages = int(input("\nEnter number of pages to scrape:\n> ") or 1)
    filename = input("\nEnter output filename (e.g., data.csv or data.xlsx):\n> ")
    table_class = input("\nEnter table class (optional, press Enter to extract all tables):\n> ").strip() or None
    
    if not table_class:
        print("\n⚠️ Warning: Extracting ALL tables. This may result in messy output!")
        proceed = input("Continue? (y/n): ").lower()
        if proceed != 'y':
            print("Operation cancelled.")
            return
    
    all_data = []
    page_range = range(1, num_pages + 1) if "{}" in base_url else [1]
    
    print(f"\n⏳ Scraping {len(page_range)} pages...")
    for page in tqdm(page_range):
        try:
            url = base_url.format(page) if "{}" in base_url else base_url
            tables = extract_tables_robust(url, table_class)
            
            if not tables:
                print(f"  ⚠️ No tables found on page {page}")
                continue
            
            for i, table in enumerate(tables):
                df = manual_table_extraction(table)
                if df is None or df.empty:
                    print(f"  ⚠️ Table {i+1} on page {page} is empty")
                    continue
                
                df['Source_URL'] = url
                df['Table_Index'] = i + 1
                if len(page_range) > 1:
                    df['Page_Number'] = page
                all_data.append(df)
                print(f"  ✅ Processed table {i+1} ({len(df)} rows)")
                
        except Exception as e:
            print(f"🚨 Page {page} failed: {str(e)}")
        time.sleep(1.5)
    
    if not all_data:
        print("\n❌ No tables extracted. Check website structure or try different class")
        return
    
    print(f"\n✅ Extracted {len(all_data)} tables with {sum(len(df) for df in all_data)} total rows")
    
    if filename.endswith('.csv'):
        pd.concat(all_data, ignore_index=True).to_csv(filename, index=False)
        print(f"💾 Saved CSV: {os.path.abspath(filename)}")
    elif filename.endswith('.xlsx'):
        with pd.ExcelWriter(filename) as writer:
            if table_class:
                pd.concat(all_data, ignore_index=True).to_excel(writer, sheet_name="Combined", index=False)
                print(f"💾 Saved Excel with combined sheet: {os.path.abspath(filename)}")
            else:
                for idx, df in enumerate(all_data):
                    page_num = df['Page_Number'].iloc[0] if 'Page_Number' in df.columns else 1
                    sheet_name = f"Page{page_num}_Table{df['Table_Index'].iloc[0]}"
                    df.to_excel(writer, sheet_name=clean_sheet_name(sheet_name), index=False)
                print(f"💾 Saved Excel with {len(all_data)} sheets: {os.path.abspath(filename)}")
    else:
        print("❌ Unsupported format. Use .csv or .xlsx")

if __name__ == "__main__":
    main()
