#!/usr/bin/env python3
"""
Robust web table extractor tool
Extracts HTML tables from websites using table IDs
"""

import requests
import pandas as pd
from bs4 import BeautifulSoup
import re
import time
import os
from tqdm import tqdm
import sys

def clean_sheet_name(name):
    """Clean invalid characters from Excel sheet names"""
    return re.sub(r'[\\/*?:\[\]]', '', name)[:30]

def manual_table_extraction(table):
    """Manually extract table data from BeautifulSoup table object"""
    # Extract headers
    headers = []
    header_row = table.find('tr')
    if header_row:
        for th in header_row.find_all(['th', 'td']):
            headers.append(th.get_text(strip=True))
    
    # Extract rows
    rows = []
    for row in table.find_all('tr')[1:]:  # Skip header row
        cols = row.find_all('td')
        if cols:  # Skip empty rows
            row_data = [col.get_text(strip=True) for col in cols]
            rows.append(row_data)
    
    # Create DataFrame
    if headers and rows:
        return pd.DataFrame(rows, columns=headers)
    elif rows:
        return pd.DataFrame(rows)
    else:
        return None

def extract_tables_robust(url, table_id=None):
    """Reliable table extraction that works for complex sites"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        if table_id:
            table = soup.find('table', id=table_id)
            tables = [table] if table else []
        else:
            tables = soup.find_all('table')
            
        return tables
    except Exception as e:
        print(f"⚠️ Error fetching {url}: {str(e)}")
        return []

def main():
    print("🔍 Xtract - Web Table Extraction Tool")
    print("=" * 50)
    
    # Get user inputs
    base_url = input("\nEnter website URL (use {} for page numbers):\n> ").strip()
    has_pagination = "{}" in base_url
    num_pages = 1
    
    if has_pagination:
        try:
            num_pages = int(input("\nEnter number of pages to scrape:\n> ") or 1)
        except ValueError:
            num_pages = 1
            print("⚠️ Using default 1 page")

    filename = input("\nEnter output filename (e.g., data.csv or data.xlsx):\n> ").strip()
    table_id = input("\nEnter table ID (optional, press Enter to extract all tables):\n> ").strip() or None
    
    if not table_id:
        print("\n⚠️ Warning: Extracting ALL tables. This may result in large output files!")
        proceed = input("Continue? (y/n): ").lower()
        if proceed != 'y':
            print("🚫 Operation cancelled")
            return
    
    all_data = []
    page_range = range(1, num_pages + 1) if has_pagination else [1]
    
    print(f"\n⏳ Scraping {len(page_range)} pages...")
    
    # Scrape pages with robust error handling
    for page in tqdm(page_range, desc="Processing Pages"):
        try:
            url = base_url.format(page) if has_pagination else base_url
            tables = extract_tables_robust(url, table_id)
            
            if not tables:
                print(f"  ⚠️ No tables found on page {page}")
                continue
            
            for i, table in enumerate(tables):
                try:
                    df = manual_table_extraction(table)
                    
                    if df is None or df.empty:
                        print(f"  ⚠️ Table {i+1} on page {page} is empty")
                        continue
                    
                    # Add metadata
                    df['Source_URL'] = url
                    df['Table_Index'] = i + 1
                    if len(page_range) > 1:
                        df['Page_Number'] = page
                    
                    all_data.append(df)
                    print(f"  ✅ Page {page}: Extracted table {i+1} ({len(df)} rows)")
                    
                except Exception as e:
                    print(f"  ⚠️ Error processing table {i+1}: {str(e)}")
            
        except Exception as e:
            print(f"🚨 Page {page} failed: {str(e)}")
        
        time.sleep(1.5)  # Delay between requests
    
    # Process and save results
    if not all_data:
        print("\n❌ No tables extracted. Possible issues:")
        print("  - Website requires JavaScript rendering")
        print("  - Table ID incorrect")
        print("  - Website blocking scrapers")
        print("  - Try using browser developer tools to inspect table structure")
        return
    
    print(f"\n✅ Extracted {len(all_data)} tables with {sum(len(df) for df in all_data)} total rows")
    
    # Save results
    output_path = os.path.abspath(filename)
    if filename.endswith('.csv'):
        combined = pd.concat(all_data, ignore_index=True)
        combined.to_csv(filename, index=False)
        print(f"💾 Saved CSV: {output_path}")
    
    elif filename.endswith('.xlsx'):
        with pd.ExcelWriter(filename) as writer:
            if table_id:
                combined = pd.concat(all_data, ignore_index=True)
                combined.to_excel(writer, sheet_name="Combined", index=False)
                print(f"💾 Saved Excel with combined sheet: {output_path}")
            else:
                for idx, df in enumerate(all_data):
                    page_num = df['Page_Number'].iloc[0] if 'Page_Number' in df.columns else 1
                    sheet_name = f"Table_{idx+1}"
                    sheet_name = clean_sheet_name(sheet_name)
                    df.to_excel(writer, sheet_name=sheet_name, index=False)
                print(f"💾 Saved Excel with {len(all_data)} sheets: {output_path}")
    else:
        print("❌ Unsupported format. Use .csv or .xlsx")

if __name__ == "__main__":
    main()
